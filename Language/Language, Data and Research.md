# 语言、数据与研究：阿檀小倪讲量化

[TOC]

## 金檀博士语音讲座“找准量化研究问题”

**研究问题的作用**

研究需要：
- theoretical framework
- research questions and/or hypotheses
- research design
- data collection

研究问题起到<u>承上启下、核心枢纽</u>的作用

从标题和研究问题来把握论文：
- 核心概念：别人之前做过的
- 核心焦点：我们即将要做的

**研究问题的来源**

- 加深认识
- 改善实践
- 改变情境

**量化研究问题的分类**

- 找多少 -> 描述性统计
- 找关系 -> 相关分析、回归分析
- 找差异 -> T检验、方差分析

主流期刊：

- Applied Linguistics
- The Modern Language Journal
- Language Learning
- Studies in Second Language Acquisition
- TESOL Quarterly
- Language Testing
- Language Teaching Research
- Language Learning and Technology
- ReCALL (关于Computer Assisted Language Learning - CALL)
- Computers and Education
- Educational Researcher

## 第1章 研究：用数据回答问题

基本内容：研究问题、数据收集、统计回答
基本理念：用数据回答问题
基本环节：故事、案例、示范

### 研究：什么是研究？什么是量化研究？

用数据回答问题：

1. 问题
2. 数据
3. 回答

**描述研究：** 我班上学生的英语成绩如何？
**相关研究：**英语成绩与所背英语单词数量之间的关系如何？
**回归研究：**作业数量、背单词数量、复习时间与学生期末考试成绩之间的关系如何？
**对比研究：**口语成绩与阅读成绩之间是否存在差异？
**比较研究：**一班、二班、三班和四班英语成绩是否存在差异？

### 问题：问题是如何形成的

**描述研究 ——** 选读物 
**相关研究 —— **做词表
**回归研究 —— **给成绩
**对比研究 —— **编教材
**比较研究 —— **定标准

研究链 =>
- 学术研究
- 教学实践 =>
  - 实践问题 =>
    - 研究问题
      - 情境要聚焦
      - 概念要核心
      - 问题要合理

### 数据：数据是如何收集的

证据性 ——数据即证据

- 对象的代表性
- 工具的有效性
- 步骤的科学性

本课程中会用到的工具：
- Range：[http://www.victoria.ac.nz/lals/about/staff/paul-nation](http://www.victoria.ac.nz/lals/about/staff/paul-nation)
  - 国际上广泛使用的词汇分析软件
  - 可根据词表来对文章中的词汇进行覆盖率和重复次数的计算
- Sketch Engine：[https://www.sketchengine.co.uk](https://www.sketchengine.co.uk)
  - 在线的语料库索引工具
  - 可基于语料库进行信息检索
  - 以词条为计算单位，提供每个词条的词类和平均降频数据信息
  - 以平均降频作为排序标准对词条进行排序
- CLPAT：[http://languagedata.net/clpat/](http://languagedata.net/clpat/)
  - 一种基于汉语水平等级字表和词表，计算汉语文本字词等级分布及句长分布的免费在线工具
  - 提供汉语文本的字词等级分布数据以及句长数据，并形成相关字词列表
- Coh-Metrix：[http://tool.cohmetrix.com/](http://tool.cohmetrix.com/)
  - 一种电脑软件，课借助计算机详细分析不同水平等级的文本连贯性和文本可读性等特征指标，并将这些数据信息与读者的语言能力背景相匹配
- Lexile：[https://www.lexile.com/](https://www.lexile.com/)
  - 一种文本难度定级工具
  - 可自动对文本难度和可读性进行评估和预测，并将这些数据与美国“共同核心州立标准计划”（the Common Core State Standards Initiative）进行衔接

### 回答：分析数据、回答问题

- 数据大通关：基本原理
- 操作跟着做：分析数据
- 图表会解读：结果解读

什么是变量？量化后的某种属性，可以用数字表示（当然，这里的变量跟编程中的变量是不一样的）

变量类型：
- 称名数据：类别，无法比较


----------


- 顺序数据：排序，可以比较，不能加减乘除
- 等距数据：有单位，没有零点，只能加减，不能乘除
- 等比数据：有单位，也有零点，可以加减乘除

什么是样本？总体当中的一部分，你可以研究的那一部分

样本的性质：
- 样本量：样本的大小
- 样本的代表性：样本和总体的相似程度

> 只有样本量和代表性都达到一定的标准，才能说明样本足够可靠。

**假设检验**

- 基础：分布

基本假设前提：
总体的数据总是会形成某一种分布形态。
其中，最常见的就是正态分布。

假设检验：
- 先对总体的特征作出某种假设（虚无假设），
- 然后决定是接受还是拒绝这个虚无假设

原理：
小概率反证法思想（小概率事件在一次试验中基本上不会发生）

数据分析：
根据样本的变量，依照统计分布思想，检验虚无假设是否显著，并推论到总体，从而回答研究问题的方法

作业：
阅读论文：http://newsmanager.commpartners.com/tesolc/downloads/features/TQ_vol47-2_webb%20macalister.pdf

## 第2章 描述：大致情况如何？

### 问题：描述性问题是如何形成的？

研究问题：
- 情景 → 聚焦
- 概念 → 提炼
- 问题 → 形成

**选读物问题**

> 参见第一章作业中要求阅读的论文，以该论文为例

_情景聚焦_：原版阅读材料中的生词太多

_提炼概念_：

如何控制生词数量呢？
- **词汇需求量**（Vocabulary Size Necessary）
- 文本覆盖率：98%

如何又能学到生词呢？
- **词汇学习机会**（Vocabulary Learning Potential)
- 词汇重复次数：最少6次

> 论文标题已经要涵盖或包括核心概念

_形成问题_：

问题1、2、3都是针对词汇需求量的；问题4是针对词汇学习机会的。

### 数据：如何进行描述性数据收集？

_对象（代表性）_：
- 国外儿童读物：选自School Journal，共计517篇文本
- 国外大人读物：选自Wellington Written Corpus，共计138篇文本
- 外语学习者分级读物：选自Oxford Bookworm，共计33篇文本

_工具_：
要搞清楚以下问题：
- 词汇需求量 → 词汇的覆盖率问题
- 词汇学习机会 → 词汇的重复次数问题

选择Range词汇分析软件
- 计算覆盖率 → 词汇需求量？
  + 特定等级词表占比
  + 假设学生先学会高频词，再学会低频词
  + Paul Nation 14级词频表
- 计算重复次数：“生词”的出现次数 → 词汇学习机会

_步骤_：
1. 建立阅读文本库
  - 儿童读物语料库（517个文本）：285,143词
  - 大人读物语料库（138个文本）：285,143词
  - 分级读物语料库（33个文本）：285,143词
2. 统计词汇覆盖率
  - 覆盖比率标准：98%
  - 儿童读物语料库 → 10,000词
  - 大人读物语料库 → 10,000词
  - 分级读物语料库 → 3,000词
3. 统计词汇重复次数
  - 主要分析出了一级词和二级词以外的词汇

![](http://ww1.sinaimg.cn/large/006tNbRwgy1ff4xuf6r1vj30zs0e4tff.jpg)

> 问题形成：
> 针对一种情况或现象描述，提出研究问题，找准最重要的一个方面或一个环节，抓住“牛鼻子”

> 数据收集：
> 样本量通常要很大，要具有广泛的代表意义，否则达不到“描述”一种广泛现象的作用


### 回答：如何进行描述性数据分析？

什么是描述统计？
用一种简单的方式，来概括和呈现一堆数据的一些基本特征

描述统计：
- 频率统计：计算个数：数数
- 集中量数：描述数据聚拢的程度：平均数
- 离散趋势：描述数据分散的程度：全距

计算词汇需求量 → 计算每种等级的词汇：频率统计

针对频率统计的SPSS操作方法：

例如，有一篇文章里面有13个单词，那么每个单词的词汇分类已知。将词汇分类录入到SPSS软件中。

![](http://ww3.sinaimg.cn/large/006tNbRwgy1ff5lvgoyczj308e0i8wf0.jpg)

然后点击工具栏中的 “分析” → “描述统计” → “频率”

![](http://ww1.sinaimg.cn/large/006tNbRwgy1ff5lx3h26yj30rq0g0tao.jpg)

选择上图中的“图表”，然后SPSS就会输出结果：

![](http://ww4.sinaimg.cn/large/006tNbRwgy1ff5lxn52syj30p80jggn9.jpg)

![](http://ww3.sinaimg.cn/large/006tNbRwgy1ff5ly05w2gj30yc0rkgok.jpg)

统计结果的解读这里是比较简单的，仔细看看就能看懂了。

## 第3章 相关：它俩有何关系

参考文献：Brezina, V., & Gablasova, D. (2015). Is there a core general vocabulary? Introducing the New General Service List. *Applied Linguistics, 36*(1), 1-22

### 相关性问题是如何形成的？

相关性研究：两个事物或者两种现象之间的相互关系进行讨论和描述

在两个事物或者两种现象中各找出一个“锚定点”，通过量化或数字化对着两个“锚定点”来进行描述。

情景：做词表
不同单词书中收录的单词不同，且单词排序也不同，不知该如何从中找出更为核心的词汇。

聚焦研究情景：锚定点：找出核心词汇，并且越核心的词汇越放在前面，因此锚定点落在了“核心”二字。

提炼核心概念：
核心？→ 日常生活中出现频率高的词汇

核心概念：
1. 通用词汇（General Vocabulary)
    - 以词频（Frequency）为衡量标准
2. 核心通用词汇（Core General Vocabulary）
    - 过去的研究：只是基于某一特定语料库的高频词来生成通用词汇，但是没有讨论不同语料库之间是否存在共同“核心部分”

词表的计算单位问题：
- 词族（Word Family)
    - 头词(Headword)加上它的曲折变化和派生变化后的形式
    - 例如：develop
    - 曲折：developed, developing ...
    - 派生：development, developer, undeveloped ...
- 词条（Lemma）
    - 头词(Headword)加上它的曲折变化后的形式，不包括派生变化后的形式
    - 例如：develop
    - 曲折：developed, developing ...

如何选择呢？参考文献的作者选择了词条Lemma作为词表单位，因为他们认为：
- 词族缺点1：对于部分语言学习者来说可能要求比较高
- 词族缺点2：语义相差较多的单词会被分到同一头词下
    - 例如：train（火车）, trainers（运动鞋）

如何形成研究问题呢？

研究问题1：Is there a substantial overlap between frequent lexical items in different general language corpora? (不同语料库的通用词汇之间是否存在大量重叠现象？)

### 数据：如何进行相关性数据收集

1. 对象（四个语料库）：
  - LOB和BE06是小规模语料库的典型代表，集中反映了不同类型的写作词汇
  - BNC是中等规模语料库的典型代表，除写作词汇外，含有10%口语词汇
  - EnTenTen12是目前国际上规模最大的语料库，其中包括大量的网络文本预料
2. 工具（Sketch Engine）
  - 为什么选择平均降频（Average Reduced Frequency）作为排序标准？例如：100篇文章中，出现“Beijing”50次（因为有两篇关于北京的文章）；出现”Beautiful“50次。这两个词的词频相同，但是日常生活中，平均来看，肯定beautiful更常用。
  - 平均降频：既考虑词频大小；又考虑词汇分布方式。如果一个单词仅仅在一小部分文本中出现的频率较高，那么它的平均降频将会较小。只有在大部分文本中出现频率越高的单词，它在词表中的排序才会越靠前。这样就有效避免某些类型文本（科普文章）中为介绍某事物而多次重复出现该单词，导致该单词的频率较高而出现在词表中较为靠前的位置。
  - Sketch Engine中对ARF的具体定义：https://www.sketchengine.co.uk/documentation/average-reduced-frequency/
3. 步骤
  - 步骤1：基于语料库生成通用词表
    - Sketch Engine在线工具
    - 基于四个语料库生成词表
    - 人工校验，排除无效词汇
    - 每个词表中各含3000词
  - 步骤2：对通用词表之间进行两两比较
    - 将四个词表两两之间进行对比
    - 找出每两个词表之间的共同词汇（AB、AC、AD、BC、BD、CD共6组）

### 回答：如何进行相关分析，从而回答问题？

不同语料库中常用词项之间是否存在重叠？

探究两个变量之间联系的紧密程度

目前解决的是相关中的线性关系；另外，相关包含以下两个属性：
- 相关强度：绝对值在0到1之间波动。0代表两个变量之间完全没有关系；1代表两个变量之间有完全的关联。
- 相关方向：正相关与负相关。正相关意味着两个变量是朝着一个方向变化；负相关意味着两个变量你进我退

相关≠因果：例如，人的身高和体重呈正相关关系，但两者之间不存在因果关系。

- 相关：A和B之间有联系
- 因果：A的变化导致B的变化

SPSS的使用：

例如，我们将四个语料库中生成的前3000个单词的词表的所有单词按排序顺序录入到SPSS中：

![](http://i2.muimg.com/588926/d37a64c2ed41ff42.png)



例如，第10行这个表示see这个单词，它在四个语料库中分别排在第10位、第10位、第11位和第9位。

![](http://i1.piimg.com/588926/69234c134347d854.png)



另外，注意在变量视图中，将度量标准改为“序号”。

然后：分析 → 相关 → 双变量

![](http://i1.piimg.com/588926/4bddaa316efa9a60.png)



将变量全部添加到右侧之后，在“相关系数”处，选择Spearman，因为Spearman是针对顺序变量的相关的。默认的Pearson将其去掉，因为Pearson是针对等比或等距变量的相关的。

结果页面：

![](http://i4.buimg.com/588926/2f2237ce7d0ca633.png)



这个统计出来的就是Spearman的rho系数。体现的是变量两两之间的相关。自己与自己之间的相关都为1；左上与右下的相关性是对称的。因为A和B的相关性与B和A的相关性相同，不存在谁前谁后的问题。

每个单元格中，

- 第一行是相关系数。相关系数后面的星号代表：在置信度（双侧）为0.01时，相关性是显著的。
- 第二行是显著性。显著性代表的是相关系数的可靠程度。相关分析的虚无假设就是：相关系数与0之间没有差异。因此，当我们算出一个相关系数之后，这个显著性越小，就代表相关系数的可靠程度越高，犯错的几率越小。
- 最后一行是样本量。

**如何呈现统计结果**

1. 对应问题：Is there a substantial overlap between frequent lexical items in different general language corpora?
2. 重制表格（重复部分删去了，并且添加了一个数据，就是两个语料库中重复词条的比例；表格中的rs代表的就是相关系数，p代表显著性）
   ![](http://i4.buimg.com/588926/e0108291d7759b5a.png)
3. 文字说明
   解释那些数字无法说清的问题，尤其是要说明相关数字之间的逻辑关系；
   进一步分析了不同语料库中不同类别的词条（比如名词、动词、介词等）之间的相关。
4. 经典例句（略）



## 第4章 回归：它们有何关系？




## 第5章 对比：它俩有何差异？




## 第6章 比较：它们有何差异？



